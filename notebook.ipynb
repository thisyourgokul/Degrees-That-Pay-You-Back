{"cells":[{"source":"## 1. Which college majors will pay the bills?\n<p><img src=\"https://assets.datacamp.com/production/project_584/img/salary.png\" width=\"400\" align=\"center\"></p>\n<p>Wondering if that Philosophy major will really help you pay the bills? Think you're set with an Engineering degree? Choosing a college major is a complex decision evaluating personal interest, difficulty, and career prospects. Your first paycheck right out of college might say a lot about your salary potential by mid-career. Whether you're in school or navigating the postgrad world, join me as we explore the short and long term financial implications of this <em>major</em> decision.</p>\n<p>In this notebook, we'll be using data collected from a year-long survey of 1.2 million people with only a bachelor's degree by PayScale Inc., made available <a href=\"http://online.wsj.com/public/resources/documents/info-Degrees_that_Pay_you_Back-sort.html?mod=article_inline\">here</a> by the Wall Street Journal for their article <a href=\"https://www.wsj.com/articles/SB121746658635199271\">Ivy League's Big Edge: Starting Pay</a>. After doing some data clean up, we'll compare the recommendations from three different methods for determining the optimal number of clusters, apply a k-means clustering analysis, and visualize the results.</p>\n<p>To begin, let's prepare by loading the following packages: <code>tidyverse</code>, <code>dplyr</code>, <code>readr</code>, <code>ggplot2</code>, <code>cluster</code>, and <code>factoextra</code>. We'll then import the data from <code>degrees-that-pay-back.csv</code> (which is stored in a folder called <code>datasets</code>), and take a quick look at what we're working with.</p>","metadata":{"dc":{"key":"4"},"run_control":{"frozen":true},"deletable":false,"editable":false,"tags":["context"]},"cell_type":"markdown"},{"execution_count":0,"outputs":[],"source":"# Load relevant packages\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(cluster)\nlibrary(factoextra)\n\n# Read in the dataset\ndegrees <- read_csv('datasets/degrees-that-pay-back.csv', col_names=c('College.Major',\n                    'Starting.Median.Salary','Mid.Career.Median.Salary','Career.Percent.Growth',\n                    'Percentile.10','Percentile.25','Percentile.75','Percentile.90'), skip=1)\n\n# Display the first few rows and a summary of the data frame\nhead(degrees)\nsummary(degrees)","metadata":{"dc":{"key":"4"},"collapsed":true,"trusted":false,"tags":["sample_code"]},"cell_type":"code"},{"source":"## 2. Currency and strings and percents, oh my!\n<p>Notice that our salary data is in currency format, which R considers a string. Let's strip those special characters using the <code>gsub</code> function and convert all of our columns <em>except</em> <code>College.Major</code> to numeric. </p>\n<p>While we're at it, we can also convert the <code>Career.Percent.Growth</code> column to a decimal value. </p>","metadata":{"dc":{"key":"11"},"run_control":{"frozen":true},"deletable":false,"editable":false,"tags":["context"]},"cell_type":"markdown"},{"execution_count":0,"outputs":[],"source":"# Clean up the data\ndegrees_clean <- degrees %>% \n    mutate_at(vars(Starting.Median.Salary:Percentile.90), \n              function(x) as.numeric(gsub(\"[\\\\$,]\",\"\",x))) %>%\n    mutate(Career.Percent.Growth = Career.Percent.Growth/100)","metadata":{"dc":{"key":"11"},"collapsed":true,"trusted":false,"tags":["sample_code"]},"cell_type":"code"},{"source":"## 3. The elbow method\n<p>Great! Now that we have a more manageable dataset, let's begin our clustering analysis by determining how many clusters we should be modeling. The best number of clusters for an unlabeled dataset is not always a clear-cut answer, but fortunately there are several techniques to help us optimize. We'll work with three different methods to compare recommendations: </p>\n<ul>\n<li>Elbow Method</li>\n<li>Silhouette Method</li>\n<li>Gap Statistic Method</li>\n</ul>\n<p>First up will be the <strong>Elbow Method</strong>. This method plots the percent variance against the number of clusters. The \"elbow\" bend of the curve indicates the optimal point at which adding more clusters will no longer explain a significant amount of the variance. To begin, let's select and scale the following features to base our clusters on: <code>Starting.Median.Salary</code>, <code>Mid.Career.Median.Salary</code>, <code>Perc.10</code>, and <code>Perc.90</code>. Then we'll use the fancy <code>fviz_nbclust</code> function from the <em>factoextra</em> library to determine and visualize the optimal number of clusters. </p>","metadata":{"dc":{"key":"18"},"run_control":{"frozen":true},"deletable":false,"editable":false,"tags":["context"]},"cell_type":"markdown"},{"execution_count":0,"outputs":[],"source":"# Select and scale the relevant features and store as k_means_data\nk_means_data <- degrees_clean %>%\n    select(Starting.Median.Salary, Mid.Career.Median.Salary, \n           Percentile.10, Percentile.90) %>%\n    scale()\n\n# Run the fviz_nbclust function with our selected data and method \"wss\"\nelbow_method <- fviz_nbclust(k_means_data, kmeans, method = \"wss\")\n\n# View the plot\nelbow_method","metadata":{"dc":{"key":"18"},"collapsed":true,"trusted":false,"tags":["sample_code"]},"cell_type":"code"},{"source":"## 4. The silhouette method\n<p>Wow, that <code>fviz_nbclust</code> function was pretty nifty. Instead of needing to \"manually\" apply the elbow method by running multiple k_means models and plotting the calculated the total within cluster sum of squares for each potential value of k, <code>fviz_nbclust</code> handled all of this for us behind the scenes. Can we use it for the <strong>Silhouette Method</strong> as well? The Silhouette Method will evaluate the quality of clusters by how well each point fits within a cluster, maximizing average \"silhouette\" width.</p>","metadata":{"dc":{"key":"25"},"run_control":{"frozen":true},"deletable":false,"editable":false,"tags":["context"]},"cell_type":"markdown"},{"execution_count":0,"outputs":[],"source":"# Run the fviz_nbclust function with the method \"silhouette\" \nsilhouette_method <- fviz_nbclust(k_means_data, kmeans, \n                                  method = \"silhouette\")\n\n# View the plot\nsilhouette_method","metadata":{"dc":{"key":"25"},"collapsed":true,"trusted":false,"tags":["sample_code"]},"cell_type":"code"},{"source":"## 5. The gap statistic method\n<p>Marvelous! But hmm, it seems that our two methods so far disagree on the optimal number of clustersâ€¦ Time to pull out the tie breaker.</p>\n<p>For our final method, let's see what the <strong>Gap Statistic Method</strong> has to say about this. The Gap Statistic Method will compare the total variation within clusters for different values of <em>k</em> to the null hypothesis, maximizing the \"gap.\" The \"null hypothesis\" refers to a uniformly distributed <em>simulated reference</em> dataset with no observable clusters, generated by aligning with the principle components of our original dataset. In other words, how much more variance is explained by <em>k</em> clusters in our dataset than in a fake dataset where all majors have equal salary potential? </p>\n<p>Fortunately, we have the <code>clusGap</code> function to calculate this behind the scenes and the <code>fviz_gap_stat</code> function to visualize the results.</p>","metadata":{"dc":{"key":"32"},"run_control":{"frozen":true},"deletable":false,"editable":false,"tags":["context"]},"cell_type":"markdown"},{"execution_count":0,"outputs":[],"source":"# Use the clusGap function to apply the Gap Statistic Method\ngap_stat <- clusGap(k_means_data, FUN = kmeans, nstart = 25, \n                    K.max = 10, B = 50)\n\n# Use the fviz_gap_stat function to vizualize the results\ngap_stat_method <- fviz_gap_stat(gap_stat)\n\n# View the plot\ngap_stat_method","metadata":{"dc":{"key":"32"},"collapsed":true,"trusted":false,"tags":["sample_code"]},"cell_type":"code"},{"source":"## 6. K-means algorithm\n<p>Looks like the Gap Statistic Method agreed with the Elbow Method! According to majority rule, let's use 3 for our optimal number of clusters. With this information, we can now run our k-means algorithm on the selected data. We will then add the resulting cluster information to label our original dataframe.</p>","metadata":{"dc":{"key":"39"},"run_control":{"frozen":true},"deletable":false,"editable":false,"tags":["context"]},"cell_type":"markdown"},{"execution_count":0,"outputs":[],"source":"# Set a random seed\nset.seed(111)\n\n# Set k equal to the optimal number of clusters\nnum_clusters <- 3\n\n# Run the k-means algorithm \nk_means <- kmeans(k_means_data, num_clusters, iter.max = 15, nstart = 25)\n\n# Add back the cluster labels to degrees\ndegrees_labeled <- degrees_clean %>%\n    mutate(clusters = k_means$cluster)","metadata":{"dc":{"key":"39"},"collapsed":true,"trusted":false,"tags":["sample_code"]},"cell_type":"code"},{"source":"## 7. Visualizing the clusters\n<p>Now for the pretty part: visualizing our results. First let's take a look at how each cluster compares in Starting vs. Mid Career Median Salaries. What do the clusters say about the relationship between Starting and Mid Career salaries?</p>","metadata":{"dc":{"key":"46"},"run_control":{"frozen":true},"deletable":false,"editable":false,"tags":["context"]},"cell_type":"markdown"},{"execution_count":0,"outputs":[],"source":"# Graph the clusters by Starting and Mid Career Median Salaries\ncareer_growth <- ggplot(degrees_labeled, aes(x=Starting.Median.Salary,y=Mid.Career.Median.Salary,\n    color=factor(clusters))) +\n    geom_point(alpha=4/5,size=6) +\n    scale_x_continuous(labels = scales::dollar) +\n    scale_y_continuous(labels = scales::dollar) +\n    xlab('Starting Median Salary') +\n    ylab('Mid Career Median Salary') +\n    scale_color_manual(name=\"Clusters\",values=c(\"#EC2C73\",\"#29AEC7\", \n                    \"#FFDD30\")) +\n    ggtitle('Clusters by Starting vs. Mid Career Median Salaries')\n\n# View the plot\ncareer_growth","metadata":{"dc":{"key":"46"},"collapsed":true,"trusted":false,"tags":["sample_code"]},"cell_type":"code"},{"source":"## 8. A deeper dive into the clusters\n<p>Unsurprisingly, most of the data points are hovering in the top left corner, with a relatively linear relationship. In other words, the higher your starting salary, the higher your mid career salary. The three clusters provide a level of delineation that intuitively supports this. </p>\n<p>How might the clusters reflect potential mid career growth? There are also a couple curious outliers from clusters 1 and 3â€¦ perhaps this can be explained by investigating the mid career percentiles further, and exploring which majors fall in each cluster.</p>\n<p>Right now, we have a column for each percentile salary value. In order to visualize the clusters and majors by mid career percentiles, we'll need to reshape the <code>degrees_labeled</code> data using tidyr's <code>gather</code> function to make a <code>percentile</code> <em>key</em> column and a <code>salary</code> <em>value</em> column to use for the axes of our following graphs. We'll then be able to examine the contents of each cluster to see what stories they might be telling us about the majors.</p>","metadata":{"dc":{"key":"53"},"run_control":{"frozen":true},"deletable":false,"editable":false,"tags":["context"]},"cell_type":"markdown"},{"execution_count":0,"outputs":[],"source":"# Use the gather function to reshape degrees and \n# use mutate() to reorder the new percentile column\ndegrees_perc <- degrees_labeled %>%\n    select(College.Major, Percentile.10, Percentile.25, \n           Mid.Career.Median.Salary, Percentile.75, \n           Percentile.90, clusters) %>%\n    gather(key=percentile, value=salary, -c(College.Major, clusters)) %>%\n    mutate(percentile=factor(percentile,levels=c('Percentile.10','Percentile.25',\n            'Mid.Career.Median.Salary','Percentile.75','Percentile.90')))","metadata":{"dc":{"key":"53"},"collapsed":true,"trusted":false,"tags":["sample_code"]},"cell_type":"code"},{"source":"## 9. The liberal arts cluster\n<p>Let's graph Cluster 1 and examine the results. These Liberal Arts majors may represent the lowest percentiles with limited growth opportunity, but there is hope for those who make it! Music is our riskiest major with the lowest 10th percentile salary, but Drama wins the highest growth potential in the 90th percentile for this cluster (so don't let go of those Hollywood dreams!). Nursing is the outlier culprit of cluster number 1, with a higher safety net in the lowest percentile to the median. Otherwise, this cluster does represent the majors with limited growth opportunity.</p>\n<p>An aside: It's worth noting that most of these majors leading to lower-paying jobs are women-dominated, according to this <a href=\"https://www.glassdoor.com/research/app/uploads/sites/2/2017/04/FULL-STUDY-PDF-Gender-Pay-Gap2FCollege-Major.pdf\">Glassdoor study</a>. According to the research:</p>\n<blockquote>\n  <p>\"The single biggest cause of the gender pay gap is occupation and industry sorting of men and women into jobs that pay differently throughout the economy. In the U.S., occupation and industry sorting explains 54 percent of the overall pay gapâ€”by far the largest factor.\" </p>\n</blockquote>\n<p>Does this imply that women are statistically choosing majors with lower pay potential, or do certain jobs pay less because women choose themâ€¦?</p>","metadata":{"dc":{"key":"60"},"run_control":{"frozen":true},"deletable":false,"editable":false,"tags":["context"]},"cell_type":"markdown"},{"execution_count":0,"outputs":[],"source":"# Graph the majors of Cluster 1 by percentile\ncluster_1 <- ....\n\n# View the plot\ncluster_1","metadata":{"dc":{"key":"60"},"collapsed":true,"trusted":false,"tags":["sample_code"]},"cell_type":"code"},{"source":"## 10. The goldilocks cluster\n<p>On to Cluster 2, right in the middle! Accountants are known for having stable job security, but once you're in the big leagues you may be surprised to find that Marketing or Philosophy can ultimately result in higher salaries. The majors of this cluster are fairly middle of the road in our dataset, starting off not too low and not too high in the lowest percentile. However, this cluster also represents the majors with the greatest differential between the lowest and highest percentiles.</p>","metadata":{"dc":{"key":"67"},"run_control":{"frozen":true},"deletable":false,"editable":false,"tags":["context"]},"cell_type":"markdown"},{"execution_count":0,"outputs":[],"source":"# Modify the previous plot to display Cluster 2\ncluster_2 <- ....\n\n# View the plot\ncluster_2","metadata":{"dc":{"key":"67"},"collapsed":true,"trusted":false,"tags":["sample_code"]},"cell_type":"code"},{"source":"## 11. The over achiever cluster\n<p>Finally, let's visualize Cluster 3. If you want financial security, these are the majors to choose from. Besides our one previously observed outlier now identifiable as Physician Assistant lagging in the highest percentiles, these heavy hitters and solid engineers represent the highest growth potential in the 90th percentile, as well as the best security in the 10th percentile rankings. Maybe those Freakonomics guys are on to somethingâ€¦</p>","metadata":{"dc":{"key":"74"},"run_control":{"frozen":true},"deletable":false,"editable":false,"tags":["context"]},"cell_type":"markdown"},{"execution_count":0,"outputs":[],"source":"# Modify the previous plot to display Cluster 3\ncluster_3 <- ....\n\n# View the plot\ncluster_3","metadata":{"dc":{"key":"74"},"collapsed":true,"trusted":false,"tags":["sample_code"]},"cell_type":"code"},{"source":"## 12. Every major's wonderful\n<p>Thus concludes our journey exploring salary projections by college major via a k-means clustering analysis! Dealing with unsupervized data always requires a bit of creativity, such as our usage of three popular methods to determine the optimal number of clusters. We also used visualizations to interpret the patterns revealed by our three clusters and tell a story. </p>\n<p>Which two careers tied for the highest career percent growth? While it's tempting to focus on starting career salaries when choosing a major, it's important to also consider the growth potential down the road. Keep in mind that whether a major falls into the Liberal Arts, Goldilocks, or Over Achievers cluster, one's financial destiny will certainly be influenced by numerous other factors including the school attended, location, passion or talent for the subject, and of course the actual career(s) pursued. </p>\n<p>A similar analysis to evaluate these factors may be conducted on the additional data provided by the Wall Street Journal article, comparing salary potential by type and region of college attended. But in the meantime, here's some inspiration from <a href=\"https://xkcd.com/1052/\">xkcd</a> for any students out there still struggling to choose a major.</p>","metadata":{"dc":{"key":"81"},"run_control":{"frozen":true},"deletable":false,"editable":false,"tags":["context"]},"cell_type":"markdown"},{"execution_count":0,"outputs":[],"source":"# Sort degrees by Career.Percent.Growth\n# .... YOUR CODE FOR TASK 12 ....\n\n# Identify the two majors tied for highest career growth potential\nhighest_career_growth <- c('....','....')","metadata":{"dc":{"key":"81"},"collapsed":true,"trusted":false,"tags":["sample_code"]},"cell_type":"code"}],"nbformat":4,"metadata":{"kernelspec":{"name":"ir","language":"R","display_name":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"3.4.1"}},"nbformat_minor":2}